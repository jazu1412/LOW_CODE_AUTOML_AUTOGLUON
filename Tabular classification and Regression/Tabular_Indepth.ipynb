{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrZWiaZzJaqpsvnYguMGeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d3cb8ee0f084f8f8686644e83d2661e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96649b96229644c8acc33dc4a6962a83",
              "IPY_MODEL_e04f6739ba674b5ba2b09269687cc8b8",
              "IPY_MODEL_4f8f0bcb4db148e4b0abf1e2420b462e"
            ],
            "layout": "IPY_MODEL_ceecf54d78c04a36ad82362dbe42237b"
          }
        },
        "96649b96229644c8acc33dc4a6962a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6e75b7bed949de8220ffc45757a14d",
            "placeholder": "​",
            "style": "IPY_MODEL_700bc2b9abbd42588a41f9f65b5efe27",
            "value": "100%"
          }
        },
        "e04f6739ba674b5ba2b09269687cc8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954f132957a4496b9e5410ee314142c4",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54dc036e520f4fbe8965eed7787728a2",
            "value": 5
          }
        },
        "4f8f0bcb4db148e4b0abf1e2420b462e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ee2c52839e741e3911df89e00186693",
            "placeholder": "​",
            "style": "IPY_MODEL_449b8b1d476244f29c19d097deb0f43a",
            "value": " 5/5 [00:05&lt;00:00,  1.12s/it]"
          }
        },
        "ceecf54d78c04a36ad82362dbe42237b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6e75b7bed949de8220ffc45757a14d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "700bc2b9abbd42588a41f9f65b5efe27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954f132957a4496b9e5410ee314142c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54dc036e520f4fbe8965eed7787728a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ee2c52839e741e3911df89e00186693": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449b8b1d476244f29c19d097deb0f43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazu1412/LOW_CODE_AUTOML_AUTOGLUON/blob/master/Tabular%20classification%20and%20Regression/Tabular_Indepth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7d3cb8ee0f084f8f8686644e83d2661e",
            "96649b96229644c8acc33dc4a6962a83",
            "e04f6739ba674b5ba2b09269687cc8b8",
            "4f8f0bcb4db148e4b0abf1e2420b462e",
            "ceecf54d78c04a36ad82362dbe42237b",
            "ed6e75b7bed949de8220ffc45757a14d",
            "700bc2b9abbd42588a41f9f65b5efe27",
            "954f132957a4496b9e5410ee314142c4",
            "54dc036e520f4fbe8965eed7787728a2",
            "1ee2c52839e741e3911df89e00186693",
            "449b8b1d476244f29c19d097deb0f43a"
          ]
        },
        "id": "Bo860orAz_Vo",
        "outputId": "d5585ad7-919b-40d5-de09-80b3050f9760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogluon.tabular[all] in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.13,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (1.12.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn<1.4.1,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (1.3.2)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (3.3)\n",
            "Requirement already satisfied: autogluon.core==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (1.1.1)\n",
            "Requirement already satisfied: autogluon.features==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (1.1.1)\n",
            "Requirement already satisfied: xgboost<2.1,>=1.6 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (2.0.3)\n",
            "Requirement already satisfied: fastai<2.8,>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (2.7.17)\n",
            "Requirement already satisfied: torch<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (2.3.1)\n",
            "Requirement already satisfied: lightgbm<4.4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (4.3.0)\n",
            "Requirement already satisfied: catboost<1.3,>=1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[all]) (1.2.7)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular[all]) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular[all]) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular[all]) (3.7.1)\n",
            "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular[all]) (1.35.18)\n",
            "Requirement already satisfied: autogluon.common==1.1.1 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular[all]) (1.1.1)\n",
            "Requirement already satisfied: ray<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.10.0)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.2.7)\n",
            "Requirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular[all]) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular[all]) (71.0.4)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]) (1.16.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (24.1)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.7.5)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.18.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (6.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (9.4.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]) (3.7.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular[all]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular[all]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular[all]) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular[all]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular[all]) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.4,>=2.2->autogluon.tabular[all]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.tabular[all]) (12.6.68)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.18 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular[all]) (1.35.18)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular[all]) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular[all]) (0.10.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.10.9.7)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (8.1.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.0.8)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.4.1)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (3.10.5)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.5.6)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.3.14)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.11.4)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.9.1)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (7.0.4)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (20.26.4)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.64.1)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (14.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.12.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (3.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular[all]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular[all]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular[all]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular[all]) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.4,>=2.2->autogluon.tabular[all]) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular[all]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular[all]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular[all]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular[all]) (3.1.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]) (9.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.4,>=2.2->autogluon.tabular[all]) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (4.0.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.23.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (13.8.1)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (4.3.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.19.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.20.0)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.19.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.65.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (2.27.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (2.16.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (4.9)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1; extra == \"all\"->autogluon.tabular[all]) (0.6.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       age workclass  fnlwgt      education  education-num  \\\n",
            "6118    51   Private   39264   Some-college             10   \n",
            "23204   58   Private   51662           10th              6   \n",
            "29590   40   Private  326310   Some-college             10   \n",
            "18116   37   Private  222450        HS-grad              9   \n",
            "33964   62   Private  109190      Bachelors             13   \n",
            "\n",
            "            marital-status        occupation    relationship    race      sex  \\\n",
            "6118    Married-civ-spouse   Exec-managerial            Wife   White   Female   \n",
            "23204   Married-civ-spouse     Other-service            Wife   White   Female   \n",
            "29590   Married-civ-spouse      Craft-repair         Husband   White     Male   \n",
            "18116        Never-married             Sales   Not-in-family   White     Male   \n",
            "33964   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
            "\n",
            "       capital-gain  capital-loss  hours-per-week  native-country   class  \n",
            "6118              0             0              40   United-States    >50K  \n",
            "23204             0             0               8   United-States   <=50K  \n",
            "29590             0             0              44   United-States   <=50K  \n",
            "18116             0          2339              40     El-Salvador   <=50K  \n",
            "33964         15024             0              40   United-States    >50K  \n",
            "Summary of occupation column: \n",
            " count              1000\n",
            "unique               15\n",
            "top        Craft-repair\n",
            "freq                142\n",
            "Name: occupation, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240913_194327\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.07 GB / 12.67 GB (79.5%)\n",
            "Disk Space Avail:   65.52 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
            "Beginning AutoGluon training ... Time limit = 120s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240913_194327\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       occupation\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
            "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       multiclass\n",
            "Preprocessing data ...\n",
            "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
            "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
            "Train Data Class Count: 13\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10315.87 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.55 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
            "\t0.2s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.25s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 796, Val Rows: 200\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'GBM': {'num_boost_round': 100, 'num_leaves': Int: lower=26, upper=66},\n",
            "\t'NN_TORCH': {'num_epochs': 10, 'learning_rate': Real: lower=0.0001, upper=0.01, 'activation': Categorical['relu', 'softrelu', 'tanh'], 'dropout_prob': Real: lower=0.0, upper=0.5},\n",
            "}\n",
            "Fitting 2 L1 models ...\n",
            "Hyperparameter tuning model: LightGBM ... Tuning model for up to 53.89s of the 119.75s of remaining time.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d3cb8ee0f084f8f8686644e83d2661e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitted model: LightGBM/T1 ...\n",
            "\t0.37\t = Validation score   (accuracy)\n",
            "\t1.02s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitted model: LightGBM/T2 ...\n",
            "\t0.355\t = Validation score   (accuracy)\n",
            "\t1.06s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitted model: LightGBM/T3 ...\n",
            "\t0.375\t = Validation score   (accuracy)\n",
            "\t0.9s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitted model: LightGBM/T4 ...\n",
            "\t0.36\t = Validation score   (accuracy)\n",
            "\t1.02s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitted model: LightGBM/T5 ...\n",
            "\t0.375\t = Validation score   (accuracy)\n",
            "\t1.04s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Hyperparameter tuning model: NeuralNetTorch ... Tuning model for up to 53.89s of the 114.06s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+\n",
            "| Configuration for experiment     NeuralNetTorch   |\n",
            "+---------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator  |\n",
            "| Scheduler                        FIFOScheduler    |\n",
            "| Number of trials                 5                |\n",
            "+---------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/AutogluonModels/ag-20240913_194327/models/NeuralNetTorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitted model: NeuralNetTorch/2c104694 ...\n",
            "\t0.355\t = Validation score   (accuracy)\n",
            "\t7.05s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitted model: NeuralNetTorch/eec1e1c9 ...\n",
            "\t0.34\t = Validation score   (accuracy)\n",
            "\t6.41s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitted model: NeuralNetTorch/1091c29c ...\n",
            "\t0.34\t = Validation score   (accuracy)\n",
            "\t9.15s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitted model: NeuralNetTorch/1433eb30 ...\n",
            "\t0.33\t = Validation score   (accuracy)\n",
            "\t9.93s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitted model: NeuralNetTorch/b10bdbc4 ...\n",
            "\t0.33\t = Validation score   (accuracy)\n",
            "\t6.75s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.75s of the 64.29s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM/T3': 0.833, 'NeuralNetTorch/1091c29c': 0.167}\n",
            "\t0.385\t = Validation score   (accuracy)\n",
            "\t0.23s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 56.0s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4982.6 rows/s (200 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240913_194327\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [' Other-service', ' Craft-repair', ' Exec-managerial', ' Sales', ' Other-service']\n",
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                      model  score_val eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0       WeightedEnsemble_L2      0.385    accuracy       0.040139  10.283412                0.001243           0.234520            2       True         11\n",
            "1               LightGBM/T3      0.375    accuracy       0.011110   0.901617                0.011110           0.901617            1       True          3\n",
            "2               LightGBM/T5      0.375    accuracy       0.017177   1.038258                0.017177           1.038258            1       True          5\n",
            "3               LightGBM/T1      0.370    accuracy       0.006070   1.015082                0.006070           1.015082            1       True          1\n",
            "4               LightGBM/T4      0.360    accuracy       0.046947   1.018968                0.046947           1.018968            1       True          4\n",
            "5               LightGBM/T2      0.355    accuracy       0.013325   1.060745                0.013325           1.060745            1       True          2\n",
            "6   NeuralNetTorch/2c104694      0.355    accuracy       0.024628   7.051543                0.024628           7.051543            1       True          6\n",
            "7   NeuralNetTorch/eec1e1c9      0.340    accuracy       0.027402   6.414190                0.027402           6.414190            1       True          7\n",
            "8   NeuralNetTorch/1091c29c      0.340    accuracy       0.027786   9.147275                0.027786           9.147275            1       True          8\n",
            "9   NeuralNetTorch/b10bdbc4      0.330    accuracy       0.023611   6.749308                0.023611           6.749308            1       True         10\n",
            "10  NeuralNetTorch/1433eb30      0.330    accuracy       0.110134   9.933926                0.110134           9.933926            1       True          9\n",
            "Number of models trained: 11\n",
            "Types of models trained:\n",
            "{'WeightedEnsembleModel', 'LGBModel', 'TabularNeuralNetTorchModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
            "('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "('int', ['bool']) : 2 | ['sex', 'class']\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240913_194327SummaryOfModels.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240913_194425\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.07 GB / 12.67 GB (79.4%)\n",
            "Disk Space Avail:   65.50 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240913_194425\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' >50K', ' <=50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10309.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** End of fit() summary ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.3s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.33s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {'num_epochs': 2},\n",
            "\t'GBM': {'num_boost_round': 20},\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 2 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
            "\t0.823\t = Validation score   (accuracy)\n",
            "\t18.99s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
            "\t0.744\t = Validation score   (accuracy)\n",
            "\t28.53s\t = Training   runtime\n",
            "\t0.58s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.823\t = Validation score   (accuracy)\n",
            "\t0.05s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 2 L2 models ...\n",
            "Fitting model: LightGBM_BAG_L2 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
            "\t0.828\t = Validation score   (accuracy)\n",
            "\t14.32s\t = Training   runtime\n",
            "\t0.31s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
            "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
            "\t0.748\t = Validation score   (accuracy)\n",
            "\t30.58s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.833, 'LightGBM_BAG_L1': 0.167}\n",
            "\t0.829\t = Validation score   (accuracy)\n",
            "\t0.07s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 113.81s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 206.6 rows/s (200 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240913_194425\")\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictClass\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.88 GB / 12.67 GB (78.0%)\n",
            "Disk Space Avail:   65.49 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=5\n",
            "Beginning AutoGluon training ... Time limit = 30s\n",
            "AutoGluon will save models to \"agModels-predictClass\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' >50K', ' <=50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10117.59 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.2s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.28s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'FASTAI': {'num_epochs': 10},\n",
            "\t'GBM': {'num_boost_round': 200},\n",
            "}\n",
            "Fitting 2 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 29.72s of the 29.71s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
            "\t0.6856\t = Validation score   (f1)\n",
            "\t27.42s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Completed 1/5 k-fold bagging repeats ...\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.71s of the -5.28s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.6856\t = Validation score   (f1)\n",
            "\t0.05s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 35.46s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 1385.5 rows/s (125 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictClass\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prior to calibration (predictor.decision_threshold=0.5):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
            "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
            "\tBase Threshold: 0.500\t| val: 0.6856\n",
            "\tBest Threshold: 0.500\t| val: 0.6856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After calibration (predictor.decision_threshold=0.5):\n",
            "decision_threshold=0.500\t| metric=\"f1\"\n",
            "\ttest_score uncalibrated: 0.6294\n",
            "\ttest_score   calibrated: 0.6294\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"accuracy\"\n",
            "\ttest_score uncalibrated: 0.8472\n",
            "\ttest_score   calibrated: 0.8472\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"balanced_accuracy\"\n",
            "\ttest_score uncalibrated: 0.7438\n",
            "\ttest_score   calibrated: 0.7438\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"mcc\"\n",
            "\ttest_score uncalibrated: 0.5457\n",
            "\ttest_score   calibrated: 0.5457\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"roc_auc\"\n",
            "\ttest_score uncalibrated: 0.8990\n",
            "\ttest_score   calibrated: 0.8990\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"precision\"\n",
            "\ttest_score uncalibrated: 0.7411\n",
            "\ttest_score   calibrated: 0.7411\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"recall\"\n",
            "\ttest_score uncalibrated: 0.5470\n",
            "\ttest_score   calibrated: 0.5470\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.500\t| metric=\"f1\"\n",
            "\ttest_score uncalibrated: 0.6294\n",
            "\ttest_score   calibrated: 0.6294\n",
            "\ttest_score        delta: 0.0000\n",
            "decision_threshold=0.250\t| metric=\"balanced_accuracy\"\n",
            "\ttest_score uncalibrated: 0.7438\n",
            "\ttest_score   calibrated: 0.8120\n",
            "\ttest_score        delta: 0.0682\n",
            "decision_threshold=0.500\t| metric=\"mcc\"\n",
            "\ttest_score uncalibrated: 0.5457\n",
            "\ttest_score   calibrated: 0.5457\n",
            "\ttest_score        delta: 0.0000\n",
            "['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
            "   age workclass  fnlwgt education  education-num       marital-status  \\\n",
            "0   31   Private  169085      11th              7   Married-civ-spouse   \n",
            "\n",
            "  occupation relationship    race      sex  capital-gain  capital-loss  \\\n",
            "0      Sales         Wife   White   Female             0             0   \n",
            "\n",
            "   hours-per-week  native-country  \n",
            "0              20   United-States  \n",
            "WeightedEnsemble_L2\n",
            "Prediction from LightGBM_BAG_L1 model:  <=50K\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing feature importance via permutation shuffling for 14 features using 5000 rows with 5 shuffle sets...\n",
            "\t34.67s\t= Expected runtime (6.93s per shuffle set)\n",
            "\t32.73s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n",
            "Persisting 2 models in memory. Models will require 0.02% of memory.\n",
            "Unpersisted 2 models: ['WeightedEnsemble_L2', 'LightGBM_BAG_L1']\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240913_194759\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.19 GB / 12.67 GB (80.4%)\n",
            "Disk Space Avail:   65.49 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ... Time limit = 30s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240913_194759\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' >50K', ' <=50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10437.96 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:  [' <=50K' ' <=50K' ' >50K' ' <=50K' ' <=50K' ' >50K' ' >50K' ' >50K'\n",
            " ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K'\n",
            " ' <=50K' ' >50K' ' >50K' ' <=50K']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.3s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "\t4.179μs\t= Feature Preprocessing Time (1 row | 10000 batch size)\n",
            "\t\tFeature Preprocessing requires 8.36% of the overall inference constraint (0.05ms)\n",
            "\t\t0.046ms inference time budget remaining for models...\n",
            "Data preprocessing and feature engineering runtime = 0.46s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 29.54s of the 29.53s of remaining time.\n",
            "\t0.725\t = Validation score   (accuracy)\n",
            "\t0.07s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "\t5.485μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t5.485μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t5.485μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t5.485μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: KNeighborsDist ... Training model for up to 29.41s of the 29.41s of remaining time.\n",
            "\t0.71\t = Validation score   (accuracy)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "\t7.238μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t7.238μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t7.238μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t7.238μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: LightGBMXT ... Training model for up to 29.28s of the 29.27s of remaining time.\n",
            "\t0.85\t = Validation score   (accuracy)\n",
            "\t0.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "\t7.106μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t7.106μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t7.106μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t7.106μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: LightGBM ... Training model for up to 28.41s of the 28.4s of remaining time.\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t0.61s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "\t5.053μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t5.053μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t5.053μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t5.053μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: RandomForestGini ... Training model for up to 27.77s of the 27.76s of remaining time.\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t1.29s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "\t0.032ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t0.032ms\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.032ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t0.032ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: RandomForestEntr ... Training model for up to 26.32s of the 26.32s of remaining time.\n",
            "\t0.835\t = Validation score   (accuracy)\n",
            "\t1.28s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "\t0.029ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t0.029ms\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.029ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t0.029ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: CatBoost ... Training model for up to 24.88s of the 24.87s of remaining time.\n",
            "\t0.86\t = Validation score   (accuracy)\n",
            "\t4.8s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "\t2.14μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t2.14μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t2.14μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t2.14μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 20.04s of the 20.04s of remaining time.\n",
            "\t0.815\t = Validation score   (accuracy)\n",
            "\t1.25s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "\t0.034ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t0.034ms\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.034ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t0.034ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 18.62s of the 18.62s of remaining time.\n",
            "\t0.82\t = Validation score   (accuracy)\n",
            "\t1.91s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "\t0.063ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t0.063ms\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.063ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t0.063ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 16.56s of the 16.56s of remaining time.\n",
            "No improvement since epoch 7: early stopping\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t2.86s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "\t0.035ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t0.035ms\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.035ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t0.035ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: XGBoost ... Training model for up to 13.64s of the 13.64s of remaining time.\n",
            "\t0.845\t = Validation score   (accuracy)\n",
            "\t0.59s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "\t3.277μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t3.277μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t3.277μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t3.277μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 13.01s of the 13.01s of remaining time.\n",
            "\t0.85\t = Validation score   (accuracy)\n",
            "\t4.7s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "\t9.122μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t9.122μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t9.122μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t9.122μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Fitting model: LightGBMLarge ... Training model for up to 8.28s of the 8.27s of remaining time.\n",
            "\t0.815\t = Validation score   (accuracy)\n",
            "\t1.26s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "\t0.027ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t0.027ms\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.027ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t0.027ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "Removing 9/13 base models to satisfy inference constraint (constraint=0.044ms) ...\n",
            "\t0.26ms\t-> 0.197ms\t(ExtraTreesEntr)\n",
            "\t0.197ms\t-> 0.19ms\t(KNeighborsDist)\n",
            "\t0.19ms\t-> 0.184ms\t(KNeighborsUnif)\n",
            "\t0.184ms\t-> 0.15ms\t(ExtraTreesGini)\n",
            "\t0.15ms\t-> 0.123ms\t(LightGBMLarge)\n",
            "\t0.123ms\t-> 0.094ms\t(RandomForestEntr)\n",
            "\t0.094ms\t-> 0.089ms\t(LightGBM)\n",
            "\t0.089ms\t-> 0.056ms\t(RandomForestGini)\n",
            "\t0.056ms\t-> 0.022ms\t(NeuralNetFastAI)\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.54s of the 6.86s of remaining time.\n",
            "\tEnsemble Weights: {'CatBoost': 1.0}\n",
            "\t0.86\t = Validation score   (accuracy)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "\t0.077μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n",
            "\t2.217μs\t = Validation runtime (1 row | 10000 batch size)\n",
            "\t0.077μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n",
            "\t2.217μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\n",
            "AutoGluon training complete, total runtime = 23.3s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26532.8 rows/s (200 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240913_194759\")\n",
            "Persisting 2 models in memory. Models will require 0.0% of memory.\n",
            "Fitting model: WeightedEnsemble_L2Best ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.6856\t = Validation score   (f1)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is able to predict 199366.1 rows per second. (User-specified Throughput = 20000.0)\n",
            "Model uses 10.0% of infer_limit time per row.\n",
            "Model satisfies inference constraint: True\n",
            "Alternative ensembles you can use for prediction: ['WeightedEnsemble_L2Best']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Deleting model WeightedEnsemble_L2Best. All files under agModels-predictClass/models/WeightedEnsemble_L2Best will be removed.\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t0.38s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
            "\t0.05s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 0.47s ... Best model: \"WeightedEnsemble_L2_FULL\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name of each refit-full model corresponding to a previous bagged ensemble:\n",
            "{'LightGBM_BAG_L1': 'LightGBM_BAG_L1_FULL', 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Distilling with teacher='WeightedEnsemble_L2_FULL', teacher_preds=soft, augment_method=spunge ...\n",
            "SPUNGE: Augmenting training data with 4000 synthetic samples for distillation...\n",
            "Distilling with each of these student models: ['LightGBM_DSTL', 'CatBoost_DSTL', 'RandomForestMSE_DSTL', 'NeuralNetTorch_DSTL']\n",
            "Fitting 4 L1 models ...\n",
            "Fitting model: LightGBM_DSTL ... Training model for up to 30.0s of the 29.99s of remaining time.\n",
            "\tWarning: Exception caused LightGBM_DSTL to fail during training... Skipping this model.\n",
            "\t\tpandas dtypes must be int, float or bool.\n",
            "Fields with bad pandas dtypes: workclass: object, education: object, marital-status: object, occupation: object, relationship: object, race: object, native-country: object\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n",
            "    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n",
            "    return lgb.train(**train_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py\", line 255, in train\n",
            "    booster = Booster(params=params, train_set=train_set)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 3433, in __init__\n",
            "    train_set.construct()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 2462, in construct\n",
            "    self._lazy_init(data=self.data, label=self.label, reference=None,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 2022, in _lazy_init\n",
            "    data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 825, in _data_from_pandas\n",
            "    _pandas_to_numpy(data, target_dtype=target_dtype),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 771, in _pandas_to_numpy\n",
            "    _check_for_bad_pandas_dtypes(data.dtypes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py\", line 763, in _check_for_bad_pandas_dtypes\n",
            "    raise ValueError('pandas dtypes must be int, float or bool.\\n'\n",
            "ValueError: pandas dtypes must be int, float or bool.\n",
            "Fields with bad pandas dtypes: workclass: object, education: object, marital-status: object, occupation: object, relationship: object, race: object, native-country: object\n",
            "Fitting model: CatBoost_DSTL ... Training model for up to 29.35s of the 29.34s of remaining time.\n",
            "\tWarning: Exception caused CatBoost_DSTL to fail during training... Skipping this model.\n",
            "\t\tfeatures data: pandas.DataFrame column 'workclass' has dtype 'category' but is not in  cat_features list\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 125, in _fit\n",
            "    X_val = Pool(data=X_val, label=y_val, cat_features=cat_features, weight=sample_weight_val)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 855, in __init__\n",
            "    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/catboost/core.py\", line 1491, in _init\n",
            "    self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n",
            "  File \"_catboost.pyx\", line 4339, in _catboost._PoolBase._init_pool\n",
            "  File \"_catboost.pyx\", line 4391, in _catboost._PoolBase._init_pool\n",
            "  File \"_catboost.pyx\", line 4200, in _catboost._PoolBase._init_features_order_layout_pool\n",
            "  File \"_catboost.pyx\", line 3083, in _catboost._set_features_order_data_pd_data_frame\n",
            "_catboost.CatBoostError: features data: pandas.DataFrame column 'workclass' has dtype 'category' but is not in  cat_features list\n",
            "Fitting model: RandomForestMSE_DSTL ... Training model for up to 29.0s of the 28.99s of remaining time.\n",
            "\tNote: model has different eval_metric than default.\n",
            "\t-0.1079\t = Validation score   (-mean_squared_error)\n",
            "\t7.85s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_DSTL ... Training model for up to 20.52s of the 20.51s of remaining time.\n",
            "\tWarning: Exception caused NeuralNetTorch_DSTL to fail during training... Skipping this model.\n",
            "\t\tFound array with 0 feature(s) (shape=(4800, 0)) while a minimum of 1 is required.\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 185, in _fit\n",
            "    train_dataset, val_dataset = self._generate_datasets(X=X, y=y, params=processor_kwargs, X_val=X_val, y_val=y_val)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 465, in _generate_datasets\n",
            "    train_dataset = self._process_train_data(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 538, in _process_train_data\n",
            "    df = self.processor.fit_transform(df)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 754, in fit_transform\n",
            "    result = self._fit_transform(X, y, _fit_transform_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 681, in _fit_transform\n",
            "    return Parallel(n_jobs=self.n_jobs)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 957, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1152, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 479, in fit_transform\n",
            "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 157, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 916, in fit_transform\n",
            "    return self.fit(X, **fit_params).transform(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/utils/categorical_encoders.py\", line 727, in fit\n",
            "    self._fit(X, handle_unknown=\"ignore\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/utils/categorical_encoders.py\", line 193, in _fit\n",
            "    X_list, n_samples, n_features = self._check_X(X)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/utils/categorical_encoders.py\", line 164, in _check_X\n",
            "    X_temp = check_array(X, dtype=None, force_all_finite=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 976, in check_array\n",
            "    raise ValueError(\n",
            "ValueError: Found array with 0 feature(s) (shape=(4800, 0)) while a minimum of 1 is required.\n",
            "Repeating k-fold bagging: 2/5\n",
            "Repeating k-fold bagging: 3/5\n",
            "Repeating k-fold bagging: 4/5\n",
            "Repeating k-fold bagging: 5/5\n",
            "Completed 5/5 k-fold bagging repeats ...\n",
            "Distilling with each of these student models: ['WeightedEnsemble_L2_DSTL']\n",
            "Fitting model: WeightedEnsemble_L2_DSTL ... Training model for up to 30.0s of the 19.95s of remaining time.\n",
            "\tEnsemble Weights: {'RandomForestMSE_DSTL': 1.0}\n",
            "\tNote: model has different eval_metric than default.\n",
            "\t-0.1079\t = Validation score   (-mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Distilled model leaderboard:\n",
            "                      model  score_val         eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      RandomForestMSE_DSTL   0.629213  mean_squared_error       0.123513  7.852762                0.123513           7.852762            1       True          5\n",
            "1  WeightedEnsemble_L2_DSTL   0.629213  mean_squared_error       0.124736  7.864723                0.001223           0.011961            2       True          6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RandomForestMSE_DSTL', 'WeightedEnsemble_L2_DSTL']\n",
            "predictions from RandomForestMSE_DSTL: [' <=50K', ' <=50K', ' >50K', ' <=50K', ' <=50K']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240913_194849\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.20 GB / 12.67 GB (80.5%)\n",
            "Disk Space Avail:   65.46 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "Presets specified: ['good_quality', 'optimize_for_deployment']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 15s of the 60s of remaining time (25%).\n",
            "\t\tContext path: \"AutogluonModels/ag-20240913_194849/ds_sub_fit/sub_fit_ho\"\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                      model  score_holdout  score_val eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0    LightGBMXT_BAG_L1_FULL       0.866071   0.862613    accuracy        0.009590          None  1.981598                 0.009590                   None           1.981598            1       True          1\n",
            "1  WeightedEnsemble_L3_FULL       0.866071   0.862613    accuracy        0.011229          None  1.985838                 0.001639                   None           0.004240            3       True          3\n",
            "2  WeightedEnsemble_L2_FULL       0.866071   0.862613    accuracy        0.011405          None  1.985766                 0.001815                   None           0.004168            2       True          2\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t39s\t = DyStack   runtime |\t21s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 21s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240913_194849\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10145.61 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.2s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.29s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 13.68s of the 20.52s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
            "\t0.859\t = Validation score   (accuracy)\n",
            "\t24.62s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 20.52s of the -9.49s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t0.859\t = Validation score   (accuracy)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting 11 L2 models ...\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 20.52s of the -9.58s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t0.859\t = Validation score   (accuracy)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 30.44s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 661.7 rows/s (125 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\t0.58s\t = Training   runtime\n",
            "Updated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 0.61s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\n",
            "Deleting model LightGBMXT_BAG_L1. All files under AutogluonModels/ag-20240913_194849/models/LightGBMXT_BAG_L1 will be removed.\n",
            "Deleting model WeightedEnsemble_L2. All files under AutogluonModels/ag-20240913_194849/models/WeightedEnsemble_L2 will be removed.\n",
            "Deleting model WeightedEnsemble_L3. All files under AutogluonModels/ag-20240913_194849/models/WeightedEnsemble_L3 will be removed.\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240913_194849\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240913_194959\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       9.91 GB / 12.67 GB (78.2%)\n",
            "Disk Space Avail:   65.46 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ... Time limit = 60s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240913_194959\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' >50K', ' <=50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10150.09 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.3s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.36s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "}\n",
            "Fitting 7 L1 models ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 59.64s of the 59.64s of remaining time.\n",
            "\t0.85\t = Validation score   (accuracy)\n",
            "\t0.78s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 58.82s of the 58.81s of remaining time.\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 58.17s of the 58.16s of remaining time.\n",
            "\t0.86\t = Validation score   (accuracy)\n",
            "\t6.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 51.27s of the 51.26s of remaining time.\n",
            "No improvement since epoch 7: early stopping\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t1.62s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 49.59s of the 49.58s of remaining time.\n",
            "\t0.845\t = Validation score   (accuracy)\n",
            "\t0.43s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ... Training model for up to 49.13s of the 49.12s of remaining time.\n",
            "\t0.85\t = Validation score   (accuracy)\n",
            "\t4.79s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 44.3s of the 44.3s of remaining time.\n",
            "\t0.815\t = Validation score   (accuracy)\n",
            "\t1.01s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.64s of the 43.15s of remaining time.\n",
            "\tEnsemble Weights: {'CatBoost': 1.0}\n",
            "\t0.86\t = Validation score   (accuracy)\n",
            "\t0.11s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 17.01s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26110.0 rows/s (200 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240913_194959\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240913_195016\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.21 GB / 12.67 GB (80.5%)\n",
            "Disk Space Avail:   65.45 GB / 107.72 GB (60.8%)\n",
            "===================================================\n",
            "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
            "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
            "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
            "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
            "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
            "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
            "Beginning AutoGluon training ... Time limit = 60s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240913_195016\"\n",
            "Train Data Rows:    1000\n",
            "Train Data Columns: 14\n",
            "Label Column:       class\n",
            "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
            "\t2 unique label values:  [' >50K', ' <=50K']\n",
            "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10450.87 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n",
            "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
            "\t\t('int', ['bool']) : 1 | ['sex']\n",
            "\t0.2s = Fit runtime\n",
            "\t14 features in original data used to generate 14 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.28s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': {},\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': {},\n",
            "\t'XGB': {},\n",
            "\t'FASTAI': {},\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Excluded models: ['KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\n",
            "Fitting 10 L1 models ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 59.72s of the 59.71s of remaining time.\n",
            "\t0.85\t = Validation score   (accuracy)\n",
            "\t0.5s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 59.19s of the 59.19s of remaining time.\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t0.64s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ... Training model for up to 58.51s of the 58.51s of remaining time.\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t1.43s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ... Training model for up to 56.91s of the 56.9s of remaining time.\n",
            "\t0.835\t = Validation score   (accuracy)\n",
            "\t1.5s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 55.26s of the 55.26s of remaining time.\n",
            "\t0.86\t = Validation score   (accuracy)\n",
            "\t5.42s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ... Training model for up to 49.82s of the 49.82s of remaining time.\n",
            "\t0.815\t = Validation score   (accuracy)\n",
            "\t0.86s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ... Training model for up to 48.82s of the 48.81s of remaining time.\n",
            "\t0.82\t = Validation score   (accuracy)\n",
            "\t0.92s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ... Training model for up to 47.76s of the 47.75s of remaining time.\n",
            "No improvement since epoch 7: early stopping\n",
            "\t0.84\t = Validation score   (accuracy)\n",
            "\t1.67s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ... Training model for up to 46.05s of the 46.04s of remaining time.\n",
            "\t0.845\t = Validation score   (accuracy)\n",
            "\t0.41s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ... Training model for up to 45.61s of the 45.61s of remaining time.\n",
            "\t0.815\t = Validation score   (accuracy)\n",
            "\t1.03s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.72s of the 44.45s of remaining time.\n",
            "\tEnsemble Weights: {'RandomForestEntr': 0.25, 'CatBoost': 0.25, 'LightGBMXT': 0.125, 'LightGBM': 0.125, 'RandomForestGini': 0.125, 'ExtraTreesEntr': 0.125}\n",
            "\t0.875\t = Validation score   (accuracy)\n",
            "\t0.17s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 15.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 609.7 rows/s (200 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240913_195016\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoGluon Tabular In-Depth Examples Completed\n"
          ]
        }
      ],
      "source": [
        "# AutoGluon Tabular - In-Depth Examples\n",
        "\n",
        "# Setup and Data Loading\n",
        "# We're preparing our tools and data to predict people's occupations based on their characteristics.\n",
        "# This is like getting ready to solve a puzzle by organizing all the pieces and understanding what picture we're trying to create.\n",
        "\n",
        "!pip install autogluon.tabular[all]\n",
        "\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "import numpy as np\n",
        "\n",
        "train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
        "subsample_size = 1000\n",
        "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "print(train_data.head())\n",
        "\n",
        "label = 'occupation'\n",
        "print(\"Summary of occupation column: \\n\", train_data['occupation'].describe())\n",
        "\n",
        "test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
        "y_test = test_data[label]\n",
        "test_data_nolabel = test_data.drop(columns=[label])\n",
        "\n",
        "metric = 'accuracy'\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "# We're fine-tuning our prediction models to make them work better for our specific occupation prediction task.\n",
        "# This is like adjusting the settings on a camera to get the clearest picture possible in different lighting conditions.\n",
        "\n",
        "from autogluon.common import space\n",
        "\n",
        "nn_options = {\n",
        "    'num_epochs': 10,\n",
        "    'learning_rate': space.Real(1e-4, 1e-2, default=5e-4, log=True),\n",
        "    'activation': space.Categorical('relu', 'softrelu', 'tanh'),\n",
        "    'dropout_prob': space.Real(0.0, 0.5, default=0.1),\n",
        "}\n",
        "\n",
        "gbm_options = {\n",
        "    'num_boost_round': 100,\n",
        "    'num_leaves': space.Int(lower=26, upper=66, default=36),\n",
        "}\n",
        "\n",
        "hyperparameters = {\n",
        "    'GBM': gbm_options,\n",
        "    'NN_TORCH': nn_options,\n",
        "}\n",
        "\n",
        "time_limit = 2*60\n",
        "num_trials = 5\n",
        "search_strategy = 'auto'\n",
        "\n",
        "hyperparameter_tune_kwargs = {\n",
        "    'num_trials': num_trials,\n",
        "    'scheduler' : 'local',\n",
        "    'searcher': search_strategy,\n",
        "}\n",
        "\n",
        "predictor = TabularPredictor(label=label, eval_metric=metric).fit(\n",
        "    train_data,\n",
        "    time_limit=time_limit,\n",
        "    hyperparameters=hyperparameters,\n",
        "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
        ")\n",
        "\n",
        "# Predictions and Evaluation\n",
        "# Now we're using our fine-tuned model to predict occupations and checking how accurate it is.\n",
        "# This is like testing our camera settings by taking pictures and seeing how clear they are.\n",
        "\n",
        "y_pred = predictor.predict(test_data_nolabel)\n",
        "print(\"Predictions: \", list(y_pred)[:5])\n",
        "perf = predictor.evaluate(test_data, auxiliary_metrics=False)\n",
        "\n",
        "# Fit Summary\n",
        "# We're getting an overview of how well our model-tuning process went and what we learned from it.\n",
        "# This is like reviewing the photos we've taken to understand what settings worked best for different situations.\n",
        "\n",
        "results = predictor.fit_summary()\n",
        "\n",
        "# Model Ensembling\n",
        "# We're combining multiple prediction models to create a \"super model\" that's more accurate than any single model.\n",
        "# This is like asking a group of experts for their opinion instead of relying on just one person's judgment.\n",
        "\n",
        "label = 'class'\n",
        "test_data_nolabel = test_data.drop(columns=[label])\n",
        "y_test = test_data[label]\n",
        "save_path = 'agModels-predictClass'\n",
        "\n",
        "predictor = TabularPredictor(label=label, eval_metric=metric).fit(train_data,\n",
        "    num_bag_folds=5, num_bag_sets=1, num_stack_levels=1,\n",
        "    hyperparameters = {'NN_TORCH': {'num_epochs': 2}, 'GBM': {'num_boost_round': 20}},\n",
        ")\n",
        "\n",
        "predictor = TabularPredictor(label=label, eval_metric='f1', path=save_path).fit(\n",
        "    train_data, auto_stack=True,\n",
        "    time_limit=30, hyperparameters={'FASTAI': {'num_epochs': 10}, 'GBM': {'num_boost_round': 200}}\n",
        ")\n",
        "predictor.leaderboard(test_data)\n",
        "\n",
        "# Decision Threshold Calibration\n",
        "# We're adjusting how confident our model needs to be before making a prediction, to balance between different types of errors.\n",
        "# This is like setting the sensitivity of a smoke alarm - too sensitive and it goes off unnecessarily, not sensitive enough and it might miss real fires.\n",
        "\n",
        "print(f'Prior to calibration (predictor.decision_threshold={predictor.decision_threshold}):')\n",
        "scores = predictor.evaluate(test_data)\n",
        "\n",
        "calibrated_decision_threshold = predictor.calibrate_decision_threshold()\n",
        "predictor.set_decision_threshold(calibrated_decision_threshold)\n",
        "\n",
        "print(f'After calibration (predictor.decision_threshold={predictor.decision_threshold}):')\n",
        "scores_calibrated = predictor.evaluate(test_data)\n",
        "\n",
        "for metric_name in scores:\n",
        "    metric_score = scores[metric_name]\n",
        "    metric_score_calibrated = scores_calibrated[metric_name]\n",
        "    decision_threshold = predictor.decision_threshold\n",
        "    print(f'decision_threshold={decision_threshold:.3f}\\t| metric=\"{metric_name}\"'\n",
        "          f'\\n\\ttest_score uncalibrated: {metric_score:.4f}'\n",
        "          f'\\n\\ttest_score   calibrated: {metric_score_calibrated:.4f}'\n",
        "          f'\\n\\ttest_score        delta: {metric_score_calibrated-metric_score:.4f}')\n",
        "\n",
        "predictor.set_decision_threshold(0.5)\n",
        "for metric_name in ['f1', 'balanced_accuracy', 'mcc']:\n",
        "    metric_score = predictor.evaluate(test_data, silent=True)[metric_name]\n",
        "    calibrated_decision_threshold = predictor.calibrate_decision_threshold(metric=metric_name, verbose=False)\n",
        "    metric_score_calibrated = predictor.evaluate(\n",
        "        test_data, decision_threshold=calibrated_decision_threshold, silent=True\n",
        "    )[metric_name]\n",
        "    print(f'decision_threshold={calibrated_decision_threshold:.3f}\\t| metric=\"{metric_name}\"'\n",
        "          f'\\n\\ttest_score uncalibrated: {metric_score:.4f}'\n",
        "          f'\\n\\ttest_score   calibrated: {metric_score_calibrated:.4f}'\n",
        "          f'\\n\\ttest_score        delta: {metric_score_calibrated-metric_score:.4f}')\n",
        "\n",
        "# Prediction Options\n",
        "# We're exploring different ways to use our trained model to make predictions about people's occupations.\n",
        "# This is like learning different techniques to use our camera, like taking single shots, burst mode, or videos, depending on what we're trying to capture.\n",
        "\n",
        "predictor = TabularPredictor.load(save_path)\n",
        "print(predictor.features())\n",
        "\n",
        "datapoint = test_data_nolabel.iloc[[0]]\n",
        "print(datapoint)\n",
        "predictor.predict(datapoint)\n",
        "\n",
        "predictor.predict_proba(datapoint)\n",
        "\n",
        "print(predictor.model_best)\n",
        "\n",
        "predictor.leaderboard(test_data)\n",
        "predictor.leaderboard(extra_info=True)\n",
        "predictor.leaderboard(test_data, extra_metrics=['accuracy', 'balanced_accuracy', 'log_loss'])\n",
        "\n",
        "i = 0\n",
        "model_to_use = predictor.model_names()[i]\n",
        "model_pred = predictor.predict(datapoint, model=model_to_use)\n",
        "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred.iloc[0]))\n",
        "\n",
        "all_models = predictor.model_names()\n",
        "model_to_use = all_models[i]\n",
        "specific_model = predictor._trainer.load_model(model_to_use)\n",
        "\n",
        "model_info = specific_model.get_info()\n",
        "predictor_information = predictor.info()\n",
        "\n",
        "y_pred_proba = predictor.predict_proba(test_data_nolabel)\n",
        "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred_proba)\n",
        "\n",
        "perf = predictor.evaluate(test_data)\n",
        "\n",
        "# Interpretability\n",
        "# We're trying to understand which characteristics are most important in predicting someone's occupation.\n",
        "# This is like figuring out which ingredients in a recipe contribute most to its taste.\n",
        "\n",
        "predictor.feature_importance(test_data)\n",
        "\n",
        "# Accelerating Inference\n",
        "# We're speeding up our prediction process so we can handle large amounts of data more quickly.\n",
        "# This is like upgrading our camera to take photos faster, so we can capture rapid action shots.\n",
        "\n",
        "predictor.persist()\n",
        "\n",
        "num_test = 20\n",
        "preds = np.array(['']*num_test, dtype='object')\n",
        "for i in range(num_test):\n",
        "    datapoint = test_data_nolabel.iloc[[i]]\n",
        "    pred_numpy = predictor.predict(datapoint, as_pandas=False)\n",
        "    preds[i] = pred_numpy[0]\n",
        "\n",
        "perf = predictor.evaluate_predictions(y_test[:num_test], preds, auxiliary_metrics=True)\n",
        "print(\"Predictions: \", preds)\n",
        "\n",
        "predictor.unpersist()\n",
        "\n",
        "# Inference Speed as a Fit Constraint\n",
        "# We're setting a speed limit for our predictions to ensure they're fast enough for real-time use.\n",
        "# This is like optimizing our photo-taking process to capture fleeting moments without blur.\n",
        "\n",
        "infer_limit = 0.00005\n",
        "infer_limit_batch_size = 10000\n",
        "predictor_infer_limit = TabularPredictor(label=label, eval_metric=metric).fit(\n",
        "    train_data=train_data,\n",
        "    time_limit=30,\n",
        "    infer_limit=infer_limit,\n",
        "    infer_limit_batch_size=infer_limit_batch_size,\n",
        ")\n",
        "\n",
        "predictor_infer_limit.persist()\n",
        "\n",
        "predictor_infer_limit.leaderboard()\n",
        "\n",
        "test_data_batch = test_data.sample(infer_limit_batch_size, replace=True, ignore_index=True)\n",
        "\n",
        "import time\n",
        "time_start = time.time()\n",
        "predictor_infer_limit.predict(test_data_batch)\n",
        "time_end = time.time()\n",
        "\n",
        "infer_time_per_row = (time_end - time_start) / len(test_data_batch)\n",
        "rows_per_second = 1 / infer_time_per_row\n",
        "infer_time_per_row_ratio = infer_time_per_row / infer_limit\n",
        "is_constraint_satisfied = infer_time_per_row_ratio <= 1\n",
        "\n",
        "print(f'Model is able to predict {round(rows_per_second, 1)} rows per second. (User-specified Throughput = {1 / infer_limit})')\n",
        "print(f'Model uses {round(infer_time_per_row_ratio * 100, 1)}% of infer_limit time per row.')\n",
        "print(f'Model satisfies inference constraint: {is_constraint_satisfied}')\n",
        "\n",
        "# Using Smaller Ensemble or Faster Model for Prediction\n",
        "# We're creating simplified versions of our model that can make predictions more quickly, trading some accuracy for speed.\n",
        "# This is like using a smaller, more portable camera that might not have all the features of a professional setup, but is quicker to use.\n",
        "\n",
        "additional_ensembles = predictor.fit_weighted_ensemble(expand_pareto_frontier=True)\n",
        "print(\"Alternative ensembles you can use for prediction:\", additional_ensembles)\n",
        "\n",
        "predictor.leaderboard(only_pareto_frontier=True)\n",
        "\n",
        "model_for_prediction = additional_ensembles[0]\n",
        "predictions = predictor.predict(test_data, model=model_for_prediction)\n",
        "predictor.delete_models(models_to_delete=additional_ensembles, dry_run=False)\n",
        "\n",
        "# Collapsing Bagged Ensembles via refit_full\n",
        "# We're simplifying our complex model into a more streamlined version that's easier to use and understand.\n",
        "# This is like taking the best aspects of multiple camera setups and combining them into one easy-to-use camera.\n",
        "\n",
        "refit_model_map = predictor.refit_full()\n",
        "print(\"Name of each refit-full model corresponding to a previous bagged ensemble:\")\n",
        "print(refit_model_map)\n",
        "predictor.leaderboard(test_data)\n",
        "\n",
        "# Model Distillation\n",
        "# We're creating a simplified version of our best model that's almost as good but much faster and easier to use.\n",
        "# This is like creating a point-and-shoot camera that captures photos almost as well as a professional setup, but is much simpler to operate.\n",
        "\n",
        "student_models = predictor.distill(time_limit=30)\n",
        "print(student_models)\n",
        "preds_student = predictor.predict(test_data_nolabel, model=student_models[0])\n",
        "print(f\"predictions from {student_models[0]}:\", list(preds_student)[:5])\n",
        "predictor.leaderboard(test_data)\n",
        "\n",
        "# Faster Presets or Hyperparameters\n",
        "# We're using pre-configured settings to quickly create models that are good enough for many purposes without requiring much setup time.\n",
        "# This is like using the automatic mode on a camera - it might not give you the absolute best picture possible, but it's quick and easy to use.\n",
        "\n",
        "presets = ['good_quality', 'optimize_for_deployment']\n",
        "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, presets=presets, time_limit=60)\n",
        "\n",
        "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, hyperparameters='very_light', time_limit=60)\n",
        "\n",
        "excluded_model_types = ['KNN', 'NN_TORCH']\n",
        "predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, excluded_model_types=excluded_model_types, time_limit=60)\n",
        "\n",
        "print(\"AutoGluon Tabular In-Depth Examples Completed\")"
      ]
    }
  ]
}